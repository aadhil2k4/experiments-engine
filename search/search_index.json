{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ExE","text":""},{"location":"#what-is-the-experiments-engine","title":"What is the Experiments Engine?","text":"<p>Experiments Engine (ExE) enables social sector orgs with digital offerings to create and monitor experiments.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":""},{"location":"#creating-an-experiment","title":"Creating an experiment","text":"<p>You can create a new experiment in ExE and generate an API key. There are two places where you need to integrate ExE: (a) when a user visits your platform and you need to decide which arm to assign to them (b) when the desired outcome is observed (e.g. user clicks a button, user completes a task, etc.)</p> <pre><code>sequenceDiagram\n    autonumber\n    actor Admin\n    Admin-&gt;&gt;ExE: Create experiment\n    Admin-&gt;&gt;ExE: Generate API key\n    ExE--&gt;&gt;Admin: API key\n    Admin-&gt;&gt;Your app: Integrate endpoint with platform</code></pre>"},{"location":"#running-the-experiment","title":"Running the experiment","text":"<p>With integration complete, you are ready to run the experiment. When a user visits your platform, your app will call the ExE API to get the arm assigned to them. You can then show them the content for that arm.</p> <p>When the outcome is observed your platform, the platform will send the outcome data back to ExE. ExE will then update the weights and results of the experiment.</p> <pre><code>sequenceDiagram\n    autonumber\n    actor User\n    User-&gt;&gt;Your app: Visit platform\n    Your app-&gt;&gt;ExE: Get arm assignment\n    ExE-&gt;&gt;Your app: Assign an arm to user\n    Your app-&gt;&gt;User: Show arm content\n    User-&gt;&gt;Your app: Interact with platform\n    Your app-&gt;&gt;ExE: Send outcome data\n    ExE-&gt;&gt;ExE: Update weights and results\n    actor Admin\n    Admin-&gt;&gt;ExE: Monitor experiment</code></pre> <p>See Setup your first experiment for a step-by-step guide.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> Intuitive UI to create and monitor experiments</li> <li> Dashboard to monitor all your experiments in one place</li> <li> Get notified when events (e.g. number of trials run, number of days passed, etc.) occur</li> <li> Track users across experiments and arms</li> <li> Multiple methods: A/B testing, multi-armed bandit, and more</li> </ul> <p>Want other features not listed here? Raise an issue in our GitHub repository or reach out to us at dsem@idinsight.org</p>"},{"location":"#why-exe","title":"Why ExE?","text":"<p>As penetration of smart phones and internet has increased in the Global South, there has been a surge in the number of digital offerings by social sector organizations.</p> <p>Platform owners have numerous ideas on how to improve user experience and increase engagement but are hampered by the lack of technical expertise and resources to test these ideas.</p> <p>We are building ExE to reduce these barriers to experimentation. We want to make it easy for platform owners to test out their ideas and make data-driven decisions. Our goal is to help orgs iterate, learn, and improve their offerings at low cost.</p>"},{"location":"#what-is-it-not","title":"What is it not?","text":"<p>To measure the impact of your offering as a whole, you may need to do an impact evaluation. This tool does not replace that. Instead, it allows your to tweak nodes in your theory of change.</p> <p>If you are looking for unbiased estimates of average treatment effects (say you want to publish an academic paper) this tool is not for you. Instead, this tool is for platform owners who want to quickly learn what works and what doesn't.</p>"},{"location":"#who-is-it-for","title":"Who is it for?","text":"<p>ExE is for social sector orgs - NGOs, social enterprises, and developing country governments - who have digital offerings and a keen desire to improve them.</p>"},{"location":"#what-kind-of-ideas-can-i-test-out","title":"What kind of ideas can I test out?","text":"<p>Any idea where can both implement and measure the outcome digitally can be tested out using ExE.</p> <p>Here are some ideas our partners are interested in testing out:</p> <ul> <li> Does the personalised AI response lead to better user engagement?</li> <li> Does using a casual tone with younger users lead to higher completion rates?</li> <li> Does reducing the number of questions asked during onboarding lead to lower drop-off rates?</li> <li> What is the best time to send messages to ensure users engage with it?</li> <li> Will this new feature lead to greater time spent on the platform?</li> </ul>"},{"location":"#design-principles","title":"Design principles","text":"<p>Here are the principles that guide the design of ExE:</p> <ul> <li>Simple: You don't need to be statistician or engineer to use ExE. It is designed for non-technical platform owners to create and monitor experiments.</li> <li>Light-weight: ExE is designed to be light-weight. We want to make it cheap for organizations to use.</li> <li>Minimal data collection: ExE should store minimal data about users.</li> <li>Open-source: ExE is open-source. We want to make it easy for organizations to use and contribute to the tool.</li> </ul>"},{"location":"#who-are-we","title":"Who are we?","text":"<p>ExE is built by a team of data scientists, engineers, and statisticians at IDinsight. IDinsight's mission is to amplify the impact of our partners by using data and evidence to inform decisions.</p>"},{"location":"deploying/","title":"Deploying","text":""},{"location":"deploying/#deploying-using-docker-compose","title":"Deploying using docker compose","text":"<p>The simplest way to deploy the application is using docker compose. The following steps will guide you through the process.</p> <p>Ensure you have Docker installed</p> <p>Step 1: Clone the ExE repository.</p> <pre><code>git clone git@github.com:IDinsight/experiments-engine.git\n</code></pre> <p>Step 2: Navigate to the <code>deployment/docker-compose/</code> subfolder.</p> <pre><code>cd deployment/docker-compose/\n</code></pre> <p>Step 3: Copy <code>template.*.env</code> files to <code>.*.env</code>:</p> <pre><code>cp template.base.env .base.env\ncp template.backend.env .backend.env\n</code></pre> <p>Edit the <code>.base.env</code> and <code>.backend.env</code> files to set the environment variables.</p> <p>Step 4: Run docker-compose</p> <pre><code>docker compose -f docker-compose.yml -p exe-stack up -d --build\n</code></pre> <p>You can now view the ExE app at <code>https://$DOMAIN/</code> (by default, this should be https://localhost/) and the API documentation at <code>https://$DOMAIN/api/docs</code> (you can also test the endpoints here).</p> <p>Step 5: Shutdown containers</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p exe-stack down\n</code></pre>"},{"location":"blog/","title":"Latest Updates","text":""},{"location":"blog/2025/04/29/auto-fail-draws/","title":"Auto fail draws","text":"<p>We rolled out a new feature where your experiment draw is marked as failure if a certain amount of time expires without a response.</p> <p>Imagine you are running an experiment to test a new kind of message and the outcome you are capturing is if the user clicked on a link.</p> <pre><code>graph LR\n    A[Experiment] --&gt;|Send Message| B[User]\n    B --&gt;|Click| C[Note success]\n    B --&gt;|No Click| ?</code></pre> <p>When the user clicks on the link, you are able to capture the success case. However, you are not capturing the failure case (user did not click).</p> <p>This leads to a situation where you have a lot of positive cases (user clicked) and no negative cases (user did not click) and it leads to a bias in the model. This might show up as the model converging too quickly to a possibly suboptimal solution.</p>"},{"location":"blog/2025/04/29/auto-fail-draws/#set-auto-fail-for-your-experiment","title":"Set auto fail for your Experiment","text":"<p>When creating a new experiment, you can now turn on \"Auto fail after\" and set a time limit. If the user does not respond within that time limit, the experiment will be marked as a failure.</p> <p></p>"},{"location":"blog/2025/04/29/auto-fail-draws/#up-next","title":"Up next","text":"<p>The proper way we'd want to handle this is by using survival analysis models. We've added that to the roadmap and hope to have it ready soon!</p>"},{"location":"blog/2025/05/08/docker-deployment/","title":"Docker deployment","text":"<p>You can now do dev and prod Docker deployments.</p> <p>We've cleaned up our docker deployments now, so you can do a dev version with hot reload, and a prod version to use the app in production.</p> <p>We've also added <code>Makefile</code> commands to make it easier to manage the Docker deplyments.</p> <p>There are more detailed instructions in the README file.</p>"},{"location":"blog/2025/04/09/new-experiment-view-page/","title":"New Experiment View page","text":"<p>We re-engineered the experiment details page to show more information at a glance. We also decided to do away with the modal and show the experiment details in a new page.</p> <p>Here's a screenshot of the new experiment view page:</p> <p></p> <p>What else would you like to see on this page? Drop us an email or raise an issue on GitHub.</p>"},{"location":"blog/2025/04/09/new-experiment-view-page/#next-up","title":"Next up","text":"<p>The \"Edit Experiment\" page.</p>"},{"location":"blog/2025/04/28/new-look-for-app/","title":"New look for app","text":"<p>We are excited to announce a new look for the app! We standardized our theme and colors across the app to make it more visually appealing and consistent.</p> <p>The new design is based on shadcn. Here's what it now looks like:</p>"},{"location":"blog/2025/04/28/new-look-for-app/#create-experiment-page","title":"Create experiment page","text":""},{"location":"blog/2025/04/28/new-look-for-app/#view-experiment-page","title":"View experiment page","text":""},{"location":"blog/2025/05/01/sticky-assignment/","title":"Sticky assignment","text":"<p>You can now draw arms with \"sticky assignment\" i.e. the same user gets the same arm throughout the experiment.</p> <p>Typically, we would randomly assign arms every time we draw an arm for a user. However, we now track whether there are repeat users, and ensure that the same user always sees the same arm.</p> <p>You can optionally turn on this feature when you configure the basic information for the experiment</p> <p></p> <p>If sticky assignments are enabled, you also need to provide a unique <code>client_id</code> for each user when you call the <code>draw_arm</code> endpoint.</p> <p></p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guides you through the types of experiments available, setting up your first experiment, etc.</p> <ul> <li> <p> Set up your first experiment</p> <p>Learn how to set up an experiment on the platform.</p> <p> Setup experiment</p> </li> <li> <p> Experiments</p> <p>Learn which types of experiments you can set up and when to use them</p> <p> Experiments</p> </li> </ul>"},{"location":"getting-started/experiments/","title":"Experiments","text":"<p>Learn abouth the types of experiments we have implemented, and how to configure them.</p> <ul> <li> <p> Multi-armed Bandits (MABs)</p> <p>Converge to the best variant quickly</p> <p> Multi-armed Bandits</p> </li> <li> <p> Contextual Bandits</p> <p>Converge to the best variant based on user profile</p> <p> Contextual Bandits</p> </li> <li> <p> Bayesian A/B Testing</p> <p>Check the effect of a \"treatment\" variant, accounting for prior knowledge</p> <p> Bayesian A/B Testing</p> </li> </ul>"},{"location":"getting-started/experiments/bayes_ab/","title":"Bayesian A/B Testing","text":"<p>Bayesian A/B testing compares two variants: treatment (e.g. a new feature) and control (e.g. an existing feature). This is a useful experiment when you need intuitive probability statements about which arm is better for making downstream decisions, and have the resources to balance how your arms are allocated to your experimental cohort. Choose this over the bandit algorithms when you're trying to make a \"permanent\" decision about which variant is better, as opposed to trying to dynamically pick the best performing variant as data comes in.</p>"},{"location":"getting-started/experiments/bayes_ab/#what-is-bayesian-ab-testing","title":"What is Bayesian A/B testing?","text":"<p>With A/B testing, you have 2 variants of a feature / implementation (one is ideally a baseline / existing feature that you want to compare the other, a new feature, against). We currently implement only 2 arms for the experiment. You present users with one of the variants at a random but with a fixed probability throughout the experiment and observe the outcome of their interaction with it. Unlike frequentist A/B testing, this method lets you set prior probabilities for the treatment and control arms, similarly to the bandit experiments. However, unlike the bandit methods, the posterior is computed at the end of the experiment, and not with every observed outcome.</p>"},{"location":"getting-started/experiments/bayes_ab/#show-me-some-math","title":"Show me some math!","text":"<p>In our current implementation of Bayesian A/B testing, we use Gaussian priors and support either real-valued or binary outcomes.</p> <p>The outcome \\(y\\) has a likelihood distribution given by: $$ \\begin{equation} y \\sim p(f(w_{\\text{treatment}}\\ \\mathbb{I}(\\text{treatment})\\ +\\ w_{\\text{control}}\\ \\mathbb{I}(\\text{control})\\ +\\ \\text{bias})) \\end{equation} $$ where \\(w_{\\text{treatment}}\\) and \\(w_{\\text{control}}\\) denote the treatment and control effect respectively. If \\(y\\) were binary-valued, \\(p\\) denotes the Bernoulli distribution, and \\(f\\) the sigmoid function. For real-valued \\(y\\), \\(p\\) is a Gaussian distribution, \\(f\\) is the identity function.</p> <p>We also assume Gaussian priors for \\(w_{\\text{treatment}}\\) and \\(w_{\\text{control}}\\):</p> \\[ \\begin{equation} w_{(\\cdot)} \\sim \\mathcal{N} (\\mathbf{\\mu}_{(\\cdot)}, \\Sigma_{(\\cdot)}) \\end{equation} \\] <p>During the course of the experiment, we choose the variant to present to the user with 50% probability for either arm.</p> <p>Once we have observed all the outcomes \\([y]_{j=1}^M\\), we can obtain the posterior distribution for the treatment and control arms using the Laplace approximation i.e. the log likelihood can be written down as follows:</p> \\[ \\begin{align*} \\log \\mathcal{L} &amp;= \\sum_{j=1}^M \\log p(y_j | f(w_{\\text{treatment}}\\ \\mathbb{I}(\\text{treatment})\\ +\\ w_{\\text{control}}\\ \\mathbb{I}(\\text{control})\\ +\\ \\text{bias})) \\\\ \\end{align*} \\] <p>Then we calculate:</p> \\[ \\begin{align*} \\mu_i^{\\text(post)} &amp;= \\theta_i^* = \\underset{\\theta_i}{\\text{argmax}} \\log \\mathcal{L}  \\\\ (\\Sigma_i^{\\text{post}})^{-1} &amp;= (\\Sigma_i^*)^{-1} = \\frac{d^2}{d\\theta_i^2} \\log \\mathcal{L} \\ \\bigg|_{\\theta_i^*} \\end{align*} \\] <p>We obtain the posterior parameters at the end of the experiment, by using MAPE<sup>1</sup> with the joint likelihood of all the observations.</p> <p>Next, we'll walk you through setting up the experiment.</p>"},{"location":"getting-started/experiments/bayes_ab/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Bayesian A/B testing</p> </li> <li> <p>The Bayesian Approach to A/B Testing</p> </li> </ol> <ol> <li> <p>Maximum a posteriori estimation (MAPE) is a method for estimating parameter valuess based on empirical data. It's similar to maximum likelihood estimation (MLE) but incorporates prior beliefs about the parameters.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/experiments/bayes_ab/setting-up/","title":"Setting up a Bayesian A/B Experiment","text":"<p>Once you've logged in and chosen to create new experiment from the landing page (see this guide), here's how to set up your Bayesian A/B experiment.</p>"},{"location":"getting-started/experiments/bayes_ab/setting-up/#fill-out-the-experiment-details","title":"Fill out the experiment details","text":"<p>Enter the details for the experiment. Both <code>Experiment Name</code> field and <code>Description</code> are mandatory. Select <code>Bayesian A/B Testing</code> as the experiment type.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/bayes_ab/setting-up/#choose-your-prior-and-outcome-type","title":"Choose your prior and outcome type","text":"<p>Choose the outcome type: either binary or real-valued.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/bayes_ab/setting-up/#create-experiment-arms","title":"Create experiment arms","text":"<p>Currently, you can have exactly 2 arms for the A/B test: a treatment and control arm.</p> <p>Enter details for each arm. The <code>Name</code> and <code>Description</code> fields are both mandatory.</p> <p>The priors are defined through the <code>Mean</code> and <code>Standard Deviation</code> parameters of the Gaussian distributions: the default values are <code>0</code> and <code>1</code> respectively.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p> <p>Once you've set up notfications and created the experiment, you can now run the experiment with your users, in the same way as you would for MABs.</p>"},{"location":"getting-started/experiments/cmabs/","title":"Contextual Bandits","text":"<p>Contextual bandits (CMABs), similarly to multi-armed bandits (MABs), are useful for running experiments where you have multiple variants of a feature / implementation that you want to test. However, the key difference is that contextual bandits take information about the end-user (e.g. gender, age, engagement history) into account while converging to the best-performing variant.</p>"},{"location":"getting-started/experiments/cmabs/#what-are-contextual-bandits-cmabs","title":"What are Contextual Bandits (CMABs)?","text":"<p>Contextual bandits work more or less in the same way as MABs do. You have \\(N\\) variants of a feature / implementation, with corresponding probability of achieving a desired outcome. As you serve beneficiaries these variants and observe the outcome of their interaction with it, you update these probabilities.</p> <p>The strategy for updating these probabilities is also similar. The crucial difference is that we take user information into account while updating these probabilities for contextual bandits. Thus, rather than having a single best-performing variant at the end of an experiment, you instead have the best-performing variant that depends on the user context.</p>"},{"location":"getting-started/experiments/cmabs/#show-me-some-math","title":"Show me some math!","text":"<p>In our current implementation of CMABs, we use Gaussian priors and support either real-valued or binary outcomes. We also allow for either real-valued or binary context values.</p> <p>Given \\(N\\) variants of a feature or implementation, and assuming that context for a user is captured by a vector \\(\\mathbf{x}\\), the outcome \\(y\\) has a likelihood distribution given by: $$ \\begin{equation} y \\sim p(f(\\theta_i \\cdot \\mathbf{x})) \\end{equation} $$ If \\(y\\) where binary-values, \\(p\\) denotes the Bernoulli distribution, \\(f\\) the sigmoid function and \\(\\theta_i \\cdot \\mathbf{x}\\) the log odds of the outcome \\(y\\) under variant \\(i (= 1, \\ldots .N)\\), dependent on the user context \\(\\mathbf{x}\\).</p> <p>Similarly, for real-valued \\(y\\), \\(p\\) is a Gaussian distribution, \\(f\\) is the identity function, and \\(\\theta_i \\cdot \\mathbf{x}\\) is the mean of the Gaussian distribution (we assume the variance to be 1 currently).</p> <p>We also assume multi-variate Gaussian priors for \\(\\theta_i\\): $$ \\begin{equation} \\theta_i \\sim \\mathcal{N} (\\mathbf{\\mu}_i, \\Sigma_i^{-1}) \\end{equation} $$</p> <p>While configuring the experiment, we require users to set a scalar value of mean \\(m_i\\) and standard deviation \\(\\sigma_i\\), which we then expand into vector mean \\(\\mu_i = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot m_i\\) and diagonal covariance matrix \\(\\Sigma = \\begin{bmatrix} \\sigma_i &amp; 0 \\\\ 0 &amp; \\sigma_i \\end{bmatrix}\\).</p> <p>During the course of the experiment, we choose the variant \\(j\\) to present to the user using Thompson sampling, dependent on the user's input context \\(\\mathbf{x}\\): $$ \\begin{equation} j = \\text{argmax}\\ [ f(\\theta_i \\cdot \\mathbf{x}]^N_{i = 1} \\end{equation} $$ after sampling \\(\\theta_i\\) from the corresponding Gaussian distribution.</p> <p>Once we have observed an outcome \\(y\\) for a given variant and user context \\(\\mathbf{x}\\), we can obtain the posterior distribution for that variant. In the case of the real-valued outcomes, the likelihood and priors are conjugate, so the update is straightforward update: i.e. given an observation \\(y\\), $$   {\\Sigma }^{(\\text{post})}_i = (\\Sigma_i + \\mathbf{x}^T \\mathbf{x}) ^ {-1} $$ $$   \\mu^{(\\text{post})}_i = \\Sigma^{\\text{post}}_i \\big(\\Sigma_i^{-1} \\mu_i + \\mathbf{x}\\ y \\big) $$ $$ \\begin{equation}   \\theta^{(\\text{post})}_i \\sim N (\\mu^{(\\text{post})}_i, (\\sigma^{(\\text{post})}_i)^2) \\end{equation} $$ This is now the new prior from which we sample while choosing variants or arms to serve the user.</p>"},{"location":"getting-started/experiments/cmabs/#laplace-approximation-for-posterior-updates-with-binary-outcomes","title":"Laplace approximation for posterior updates with binary outcomes","text":"<p>When the outcome \\(y\\) is binary valued (and therefore has a Bernoulli likelihood), the Gaussian priors for \\(\\theta_i\\) and the likelihood are non-conjugate. This means that we cannot update the posteriors in closed form as in the equations above.</p> <p>We will instead use the Laplace approximation i.e. we will assume that the posterior distribution is a Gaussian whose mean is given by the maximum a posteriori (MAP) estimate \\(\\theta_i^*\\), and whose covariance is given by the inverse Hessian of the Bernoulli log likelihood, evaluated at \\(\\theta_i^*\\) i.e. for \\(M\\) context-observations pairs \\(\\{\\mathbf{x}, y\\}_{j=1}^M\\) the log likelihood can be written down as follows:</p> \\[ \\begin{align*} \\log \\mathcal{L} &amp;= \\sum_{j=1}^M \\log p(y_j | f(\\theta_i \\cdot \\mathbf{x}_j)) \\\\ &amp;= \\sum_j y_j \\log(\\theta_i \\cdot \\mathbf{x}_j) + (1 - y_j) \\log (1 - \\theta_i \\cdot \\mathbf{x}_j) \\end{align*} \\] <p>Then we calculate:</p> \\[ \\begin{align*} \\mu_i^{\\text(post)} &amp;= \\theta_i^* = \\underset{\\theta_i}{\\text{argmax}} \\log \\mathcal{L}  \\\\ (\\Sigma_i^{\\text{post}})^{-1} &amp;= (\\Sigma_i^*)^{-1} = \\frac{d^2}{d\\theta_i^2} \\log \\mathcal{L} \\ \\bigg|_{\\theta_i^*} \\end{align*} \\] <p>We update these parameters by repeating the MAP estimation every time a new outcome is logged.</p> <p>Next, we'll show you how to set up the CMAB, and configure the contexts and outcomes.</p>"},{"location":"getting-started/experiments/cmabs/#addiitional-resources","title":"Addiitional Resources","text":"<p>You can learn more about contextual bandits from the following resources:</p> <ol> <li> <p>Contextual Bandits</p> </li> <li> <p>Reinforcement Learning: An Introduction, Sutton and Barto, Chapter 2, Section 2.9</p> </li> </ol>"},{"location":"getting-started/experiments/cmabs/run-experiment/","title":"Running a CMAB experiment","text":"<p>Once the experiment is set up, you have a unique <code>experiment_id</code> assigned to it. You can now use this experiment ID to randomly choose an arm or variant to present to each new end-user by calling the <code>draw_arm</code> endpoint.</p> <p></p> <p>This works similarly to the <code>draw_arm</code> endpoint for MABs, but crucially require the context to be input in addition to the <code>experiment_id</code>. You can also optionally input a unique <code>draw_id</code>, or the system will autogenerate one.</p> <p>You can then use this ID to record the outcome of presenting the arm you drew to the user, using the <code>update_arm</code> endpoint, similarly to how it works with MABs.</p> <p></p> <p>We're currently working on visuals for an experiment card corresponding to CMABs and on a dashboard to display more detailed information across experiments.</p>"},{"location":"getting-started/experiments/cmabs/setting-up/","title":"Setting up a Contextual Bandit","text":"<p>Once you've logged in and chosen to create new experiment from the landing page (see this guide), here's how to set up your CMAB.</p>"},{"location":"getting-started/experiments/cmabs/setting-up/#fill-out-the-experiment-details","title":"Fill out the experiment details","text":"<p>Enter the details for the experiment. Both <code>Experiment Name</code> field and <code>Description</code> are mandatory. Select <code>Contextual Bandit</code> as the experiment type.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/cmabs/setting-up/#choose-your-prior-and-outcome-type","title":"Choose your prior and outcome type","text":"<p>Choose the outcome type: binary or real-valued (the prior is fixed to a normal distribution).</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/cmabs/setting-up/#configure-user-context","title":"Configure user context","text":"<p>Enter the user information that you want to track for the experiment. Each context corresponds to one \"feature\" of the user. The <code>Name</code> and <code>Description</code> fields are mandatory, and the context value can either be real-valued (e.g. age) or binary (e.g. whether or not a user has children).</p> <p></p> <p>Then click <code>Next</code> to proceed.</p>"},{"location":"getting-started/experiments/cmabs/setting-up/#create-experiment-arms","title":"Create experiment arms","text":"<p>Click on the <code>+ Add Arm</code> button to add arms to the experiment. You can add as many arms as you like but must have a minimum of 2 arms.</p> <p>Enter details for each arm. The <code>Name</code> and <code>Description</code> fields are both mandatory.</p> <p>The priors are defined through the <code>Mean</code> and <code>Standard Deviation</code> parameters of the Gaussian distributions: the default values are <code>0</code> and <code>1</code> respectively.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p> <p>Once you've set up notfications and created the experiment, you can now run the experiment with your users.</p>"},{"location":"getting-started/experiments/mabs/","title":"Multi-Armed Bandits","text":"<p>Multi-armed Bandits (MABs) are useful for running experiments where you have multiple variants of a feature / implementation that you want to test, and want to automatically converge to the variant that produces the best results.</p>"},{"location":"getting-started/experiments/mabs/#what-are-multi-armed-bandits-mabs","title":"What are Multi-Armed Bandits (MABs)?","text":"<p>MABs are a specialized reinforcement learning algorithm: let's imagine that you have set up \\(N\\) variants of an experiment, and for each variant you some prior probability of a desired result. You serve your each of your users one of these variants (the strategy for choosing the variant is based on the prior probabilities), and observe the result of their interaction with it: for example, if you are experimenting with a website interface, you might want to track when a user returns to a page, clicks on a link, etc..</p> <p>Once you have observed the result, the algorithm updates your arm / variant's probability of achieving the desired result. The next time you serve a user one of the variants, the experiments engine uses these updated probabilities to determine which variant to show them.</p> <p>Since we update the probabilities for the variants with every result observation, at any given time you can observe the updated probability of success for every arm. The best-performing variant at the end of the experiment is the one with the highest probability. For example, in the plot below variant 2 is the best-performing one at the end of 200 observations.</p> <p></p>"},{"location":"getting-started/experiments/mabs/#show-me-some-math","title":"Show me some math!","text":"<p>In our current implementation of MABs, we support the following configurations of priors and outcome distributions:</p> Prior Outcome Update algorithm Beta Binary (Bernoulli distribution) Thompson sampling Gaussian Real-valued (Gaussian distribution) Thompson sampling"},{"location":"getting-started/experiments/mabs/#beta-binomial-bandits","title":"Beta-binomial bandits","text":"<p>Given \\(N\\) variants of a feature or implementation, if we assume that the observed result \\(y\\) of these experiments is binary-valued, we can model the result likelihood as follows: $$ \\begin{equation} y \\sim \\text{Bernoulli} (\\theta_i) \\end{equation} $$ where \\(\\theta_i\\) denotes the binomial probability of the outcome \\(y\\) under variant \\(i (= 1, \\ldots .N)\\)</p> <p>We also assume Beta priors for \\(\\theta_i\\): $$ \\begin{equation} \\theta_i \\sim \\text{Beta} (\\alpha_i, \\beta_i) \\end{equation} $$ We require users to set \\(\\alpha_i\\) and \\(\\beta_i\\) parameters for each variant, before the experiment begins. The mean of the distribution is \\(\\frac{\\alpha_i}{\\alpha_i + \\beta_i\\)} -- this means that a larger value of \\(\\alpha_{\\cdot}\\) corresponds to a higher probability of success, and \\(\\beta_{\\cdot}\\) is inversely proportional to the probability of success.</p> <p>During the course of the experiment, we choose the variant \\(j\\) to present to the user using Thompson sampling: $$ \\begin{equation} j = \\text{argmax}\\ [ \\theta_i ]^N_{i = 1} \\end{equation} $$</p> <p>Finally, once we have observed an outcome \\(y\\) for a given variant, we can obtain the posterior distribution for that variant. Since the beta and binomial distributions are conjugate, this is a relatively straightfoward update: i.e. given an observation \\(y\\), $$ \\begin{equation}   \\theta^{(\\text{post})}_i \\sim \\text{Beta} (\\alpha_i + y, \\beta_i + (1 - y)) \\end{equation} $$ This then becomes the new prior from which we sample variants while choosing arms / variants.</p>"},{"location":"getting-started/experiments/mabs/#gaussian-priors-and-real-valued-outcomes","title":"Gaussian priors and real-valued outcomes","text":"<p>With real-valued outcomes and Gaussian priors, the likelihood becomes: $$ \\begin{equation}   y \\sim \\mathcal{N} (\\theta_i, \\sigma^2) \\end{equation} $$</p> <p>In our current implementation we set \\(\\sigma = 1\\). The corresponding priors for each variant are: $$ \\begin{equation}   \\theta_i \\sim \\mathcal{N} (\\mu_i, \\sigma^2_i) \\end{equation} $$</p> <p>Thompson sampling proceeds similar to Eqn (3) and the posterior update given an observation \\(y\\) is as follows: $$   {\\sigma }^{(\\text{post})}_i = \\frac{1}{\\sqrt{\\frac{1}{\\sigma^2_i} + \\frac{1}{\\sigma^2}}} $$ $$   \\mu^{(\\text{post})}_i = \\frac{\\frac{\\mu_i}{\\sigma_i^2} + \\frac{y}{\\sigma^2}}{\\frac{1}{\\sigma^2_i} + \\frac{1}{\\sigma^2}} $$ $$ \\begin{equation}   \\theta^{(\\text{post})}_i \\sim N (\\mu^{(\\text{post})}_i, (\\sigma^{(\\text{post})}_i)^2) \\end{equation} $$</p> <p>Now, you can set up the MAB, and configure the priors for the variants and outcomes.</p>"},{"location":"getting-started/experiments/mabs/#additional-resources","title":"Additional Resources","text":"<p>You can learn more about multi-armed bandits from the following resources:</p> <ol> <li> <p>Multi-armed Bandits with Thompson Sampling</p> </li> <li> <p>Reinforcement Learning: An Introduction, Sutton and Barto, Chapter 2</p> </li> </ol>"},{"location":"getting-started/experiments/mabs/run-experiment/","title":"Running a MAB experiment","text":"<p>Once the experiment is set up, you have a unique <code>experiment_id</code> assigned to it. You can now use this experiment ID to randomly choose an arm or variant to present to each new end-user by calling the <code>draw_arm</code> endpoint.</p> <p></p> <p>You can either input a unique <code>draw_id</code> or the endpoint generates one for you.</p> <p>You can then use this ID to record the outcome of presenting the arm you drew to the user, using the <code>update_arm</code> endpoint.</p> <p></p> <p>The <code>update_arm</code> endpoint updates the prior probability of the corresponding arm based on the observed outcome.</p> <p>As you observe more and more outcomes, you can track the experiment's progress using Experiment Cards and the notfications.</p> <p></p> <p></p> <p>We're also working on a dashboard to display more detailed information across experiments!</p>"},{"location":"getting-started/experiments/mabs/setting-up/","title":"Setting up a Multi-Armed Bandit","text":"<p>Once you've logged in and chosen to create new experiment from the landing page (see this guide), here's how to set up your MAB.</p>"},{"location":"getting-started/experiments/mabs/setting-up/#fill-out-the-experiment-details","title":"Fill out the experiment details","text":"<p>Enter the details for the experiment. Both <code>Experiment Name</code> field and <code>Description</code> are mandatory. Select <code>MAB</code> as the experiment type.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/mabs/setting-up/#choose-your-prior-and-outcome-type","title":"Choose your prior and outcome type","text":"<p>Choose the prior distribution and the corresponding outcome. Currently, we only support conjugate distributions, this means that the allowed prior-outcome combos are \"Beta\" / \"Binary\" and \"Normal\" / \"Real-valued\".</p> <p> </p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/experiments/mabs/setting-up/#create-experiment-arms","title":"Create experiment arms","text":"<p>Click on the <code>+ Add Arm</code> button to add arms to the experiment. You can add as many arms as you like but must have a minimum of 2 arms.</p> <p>Enter details for each arm. The <code>Name</code> and <code>Description</code> fields are both mandatory.</p> <p>If you chose the Beta prior for the arms, the priors are defined through the <code>alpha</code> and <code>beta</code> parameters of the Beta distribution: the default values are <code>1</code> and <code>1</code> respectively. The higher the <code>alpha</code> value, the more certain your prior on the arm achieving \"success\", while the reverse is true for the <code>beta</code> parameter[^1].</p> <p>[^1] We know this is unintuitive for non-statisticians -- we're working on visual for configuring priors!</p> <p></p> <p>Similarly, for the Normal prior for the arms, the priors are defined through the <code>mean</code> and <code>standard deviation</code> parameters of the Gaussian distributions: the default values are <code>0</code> and <code>1</code> respectively. A higher <code>mean</code> value indicates a higher expected outcome for the corresponding arm, while a higher <code>standard deviation</code> indicates higher variability in the outcome for this arm.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p> <p>Once you've set up notfications and created the experiment, you can now run the experiment with your users.</p>"},{"location":"getting-started/first-experiment/","title":"Set up your first experiment","text":"<p>This shows you how to setup a multi-armed bandit (MAB) on the platform. It will show you how to create a new experiment, add multiple arms, integrate with your app, and track the results.</p> <ul> <li> <p> Create your experiment</p> <p>Setup a multi-armed bandit experiment with 3 arms.</p> <p> Create an experiment</p> </li> <li> <p> Integrate with your application</p> <p>Get your API key. Setup your app to call the right endpoints</p> <p> Integration</p> </li> <li> <p> Monitor your experiment</p> <p>See how your experiment is doing. Track the results.</p> <p> Monitor</p> </li> </ul>"},{"location":"getting-started/first-experiment/create/","title":"Create an experiment","text":""},{"location":"getting-started/first-experiment/create/#log-in-to-the-application","title":"Log in to the application","text":"<p>If you have deployed the docker containers locally as per the instructions here, you should see a login screen when you navigate to <code>https://localhost/</code></p> <p></p> <p>The credentials are what you set in your <code>.backend.env</code> file.</p>"},{"location":"getting-started/first-experiment/create/#create-an-experiment_1","title":"Create an experiment","text":"<p>Once you have logged in, you should see a screen like this:</p> <p></p> <p>Click on the <code>+ New Experiment</code> button to create a new experiment.</p>"},{"location":"getting-started/first-experiment/create/#fill-out-the-experiment-details","title":"Fill out the experiment details","text":"<p>Enter the details for the experiment. Both <code>Experiment Name</code> field and <code>Description</code> are mandatory. Select <code>MAB</code> as the experiment type.</p> <p></p> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/first-experiment/create/#choose-the-treatment-and-outcome-type","title":"Choose the treatment and outcome type","text":"<p>Here you specify if your outcome is binary (e.g. Yes/No, Upvote/Downvote) or continuous (e.g. time spent)</p> <p></p>"},{"location":"getting-started/first-experiment/create/#create-experiment-arms","title":"Create experiment arms","text":"<p>Click on the <code>+ Add Arm</code> button to add arms to the experiment. You can add as many arms as you like but must have a minimum of 2 arms.</p> <p></p> <p>Enter details for each arm. The <code>Name</code> and <code>Description</code> fields are both mandatory. You can also change the prior (1) for the arms.</p> <ol> <li>This MAB is using Thompson Sampling. The priors are defined through the <code>alpha</code> and <code>beta</code> parameters of the Beta distribution. The default values are <code>1</code> and <code>1</code> respectively. The higher the values, the more certain the arm is. The probability of success of the arm is calculated as <code>alpha / (alpha + beta)</code>. Side note: We realize this is not intuitive for the non-statistician. Look out for a visual way to set the priors in the future.</li> </ol> <p>Click on the <code>Next</code> button to proceed.</p>"},{"location":"getting-started/first-experiment/create/#set-notifications","title":"Set notifications","text":"<p>You can select events when you should be notified. Let's select the first two and set values as in the image below.</p> <p></p> <p>Click on the <code>+ Create Experiment</code> to create the experiment.</p>"},{"location":"getting-started/first-experiment/create/#experiment-created","title":"Experiment created","text":"<p>You will be taken to the home page where you can see the details of the experiment and the arms you created.</p> <p></p> <p>You can also click on the card to get a detailed view of the experiment.</p> <p></p>"},{"location":"getting-started/first-experiment/create/#next-steps","title":"Next steps","text":"<p>Now we are ready to integrate this with your application. Go to Integrate with your application to see how to do that.</p>"},{"location":"getting-started/first-experiment/integrate/","title":"Integrate with your app","text":""},{"location":"getting-started/first-experiment/integrate/#go-to-the-integration-page","title":"Go to the Integration page","text":"<p>Once you have created an experiment, you can integrate it with your app. Click on Integration in the menu to get to this page</p> <p></p>"},{"location":"getting-started/first-experiment/integrate/#create-a-new-key","title":"Create a new key","text":"<p>Click on \"Recreate Key\" to create a new key. You should see a new key appear as per the image below</p> <p></p> <p>This key is used to authenticate your app with the API. You should keep it secret and not share it with anyone. This is the only time you will see this key, so make sure to copy it somewhere safe.</p>"},{"location":"getting-started/first-experiment/integrate/#test-out-the-endpoints","title":"Test out the endpoints","text":"<p>You can now use Swagger to test out the API endpoints. Go to <code>https://localhost/api/docs</code> to see the Swagger UI. You will need to enter the key you just created to authenticate.</p> <p>![API]../(../images/create_experiment/api.png)</p>"},{"location":"getting-started/first-experiment/monitor/","title":"Monitor your experiment","text":"<p>Stay tuned! We are working on this section.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""}]}